---
title: "Applied Linear Algebra"
author: "Alex Stephenson"
date: "7/3/2020"
output: 
    html_document:
        toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 1: Vectors 

A *vector* is an ordered list of numbers. 

```{r}
vec <- c(-1.1, 0.0, 3.6, -7.2)
vec
```

Vectors are equal $a = b$ when they have the same size and same corresponding entries 

```{r}

# Equal 
a <- c(1,2,3)
b <- c(1,2,3)

a == b

# Not Equal 
a2 <- c(1,2,4)
b2 <- c(2,5,6)

a2 == b2
```

Block (or stacked) vectors are made by concatenating two or more vectors 

```{r}
stack <- c(a,b)
stack
```

Subvectors are made by slicing a vector into sub-elements. Usually colon notation is used $a_{r:s}$ where the subscript is the *index range*. While most computer languages index starting at 0, R follows standard mathematical notation where vectors are indexed beginning at 1. 

```{r}
z <- c(1, -1, 2,0)
z[2:3]
```

Zero vectors are vectors with all elements equal to 0. 

```{r}
zeros <- rep(0, 4)
zeros
```
Unsurprisingly, the Ones vector is a vector with all elements equal to 1 

```{r}
ones <- rep(1, 4)
ones
```

Sparsity is an attribute of a vector and a vector is said to be sparse if many of its entries are zero. The number of nonzero entries of a n vector is denoted `nnz()`

```{r}
nnz <- function(vector){
    return(length(vector[vector == 0]))
}

nnz(c(1,2,0,0))
```

### Vector Addition 

Two vectors of the same size can be added together by adding the corresponding elements. This forms a new vector of the same size, called the *sum* of the vectors 

```{r}
v1 <- c(0,7,3)
v2 <- c(1,2,0)

v1 + v2

```

Subtraction is a different form of addition so works the same way 

```{r}
v1 <- c(1,9)
v2 <- c(1,1)

v1 - v2
```

Being a computer language R can do something funky when adding different length vectors 

```{r}
v1 <- c(1,2,3)
v2 <- c(1,2)

# The result here is due to "recycling" which is occasionally useful, but will lead to incorrect computations if not watched out for. 
v1 + v2
```

#### Properties of Vector Addition 

1. Commutative: $a + b = b + a$
2. Associative: $(a + b) + c = a + (b + c) = a + b + c$
3. Adding the zero vector leads to to no change in the original vector. 
4. Subtracting a vector from itself yields the 0 vector 

### Scalar-vector multiplication 

*Scalar multiplication* is when each element of a vector is multiplied by a scalar. A scalar means a number like 1, 46, or 3/5. 

```{r}
-2*c(1,9,6)
```

#### Properties 

1. Commutative: $\alpha a = a\alpha$
2. Associative: $(\beta\gamma)a = \beta(\gamma a)$
3. If a is a vector, $(\beta + \gamma)a = \beta a + \gamma a$
```{r}
(2 + 5)*v1 == 2*v1 + 5*v1
```

### Inner Product 

The standard *inner product* (aka "dot product") of two n-vectors is defined as the the scalar $a^Tb = a_1b_1 + ... + a_nb_n$ or the sum or the products of corresponding entries. 

```{r}
innerprod <- function(vec1, vec2){
    # Note that t() is the transpose function in R
    return(t(vec1)%*%vec2)
}

innerprod(c(-1,2,2), c(1,0,-3))
```

#### Properties 

1. Commutativity: $a^Tb = b^Ta$. This means that order does not matter for the arguments in the inner product. 
2. Associatitivy with scalar multiplication: $(\gamma a)^Tb = \gamma(a^Tb) = \gamma a^Tb$
3. Distributivity with vector additions $(a + b)^Tc = a^Tc + b^Tc$
4. "Expand the product": $(a + b)^T(c+d) = a^Tc + a^Td + b^Tc + b^Td$. Note that on the left hand side the "+" means vector addition and on the right the sums means scalar addition. 

For block vectors, if vectors *a* and *b* are block vectors and the corresponding blocks have the same size (they're conformable) then: $a^Tb = a_1^Tb_1 + ... a_k^Tb_k$

The inner product of the block vectors is the sum of the inner products of the block. 

```{r}
#We can use our inner product function to calculate co-cocurrances of two vectors 
# The number of objects in both A and B 
innerprod(c(0,1,1,1,1,1,1), c(1,0,1,0,1,0,0))
```
An example of where the inner product can be useful is giving the expected value of a quantity 

```{r}
exp_value <- function(probabilities, values){
    return(innerprod(probabilities, values))
}
values <- c(-1, 0, 5000)
probabilities <- c(.80, .15, .05)

exp_value(probabilities, values)


```
### Exercises

Suppose a class grade is determined by 10 assignments, where 8 are homework assignments on a 0-10 scale, one is a midterm on a 0-120 scale, and one is a final on a 0-160. The student's total course grade is a weighted average on a 0-100 scale is based on 25% homework, 35% midterm and 40% final. 

```{r}
calculate_course_grade <- function(hw, mid, final, scale){
    # We'll assume that scale is the weights 
    # which go in homework, midterm, final order 
    
    # Convert homework to 0-100 scale 
    # Because there are 8 assignments, we need 10/8
    # as a weight 
    hw_sum <- sum(10/8 * hw)
    
    # Convert midterm to 0-100 scale 
    # Midterm grade is out of 120 
    mid <- 100/120 * mid 
    
    # Convert final to 0-100 scale 
    # Final Grade is out of 160 
    final <- 100/160 * final 
    grades <- c(hw_sum, mid, final )
    
    # put into grade order 
    return(innerprod(grades, scale))
}

hw <- c(8,9, 5, rep(10,4))
mid <- 90
final <- 130
scale <- c(.25, .35, .4)
calculate_course_grade(hw, mid, final, scale)

```
#### Notes 

In R, we can use `crossproduct()` instead of our inner product function. 

## Chapter 2: Linear Functions 

### Defining Linear and Affine Functions 
$f:R^n \rightarrow R$ means that $f$ is a function that maps real n-vectors to real numbers. A function $f:R^n \rightarrow R$ must specify what value $f$ takes for any possible argument $x \in R^n$

If $x$ is an n-vector then $f(x)$ which is a scalar denotes the *value* of the function $f$ at $x$. 

We can also interpret $f$ as a function of n scalar arguments in which case $f(x) = f(x_1,..., x_n)$.

**The Inner product function**: Suppose $a$ is an n-vector, then the inner product function for any n-vector x is: $f(x) = a^Tx = a_1x_1 +...+a_nx_n$. 

- We can think of $f$ as a weighted sum of the elements of x where the elements of a give the weights.
- Note that if a function is linear then it can be expressed as the inner product of its argument with some fixed vector

**Superposition and Linearity**: The inner product function satisfies 

$$\begin{aligned}
f(\alpha x + \beta y) &= a^T(\alpha x + \beta y) \\
f(\alpha x + \beta y) &= a^T(\alpha x) + a^T(\beta y) \\
f(\alpha x + \beta y) &= \alpha(a^T) + \beta(a^Ty) \\
f(\alpha x + \beta y) &= \alpha f(x) + \beta f(y) \\
\end{aligned}$$

for all n-vectors x, y and all scalars $\alpha, \beta$. This property is called *superposition*. Any function that has this property is called *linear*. 

Superposition can be broken down into two properties. A function $f: R^n \rightarrow R$ is linear if it satisfies: 

- Homogeneity: For any n-vector x and any scalar $\alpha$ $f(\alpha x) = \alpha f(x)$. 
- Additivity: For any n-vectors x, y: $f(x+y) = f(x) + f(y)$

**Affine Function**: A linear function plus a constant. Formally, a function $f: R^n \rightarrow R$ is affine $iff$ it can be expressed $f(x) = a^Tx + b$ for some n-vector a and scalar b. 

- b is sometimes called the offset
- Any affine scalar valued function satisfies the variation on superposition $f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)$ for all n-vectors x,y, and all scalars $\alpha, \beta$ that satisfy $\alpha + \beta = 1$
    - For linear functions superposition holds for any coefficient pair $\alpha, \beta$
    
### Taylor Approximation 

**Taylor Approximation**: Suppose that $f: R^n \rightarrow R$ is differentiable (its partial derivatives exist). Let $z$ be an n-vector. The first order *Taylor Approximation* of $f$ near the point $z$ is the function $\hat{f}(x)$ defined as: 

$\hat{f}(x) = f(z) + \frac{\partial f}{\partial x_1}(z)(x_1 - z_1) + ... \frac{\partial f}{\partial x_n}(z)(x_n - z_n)$

- A Taylor approximation is an affine function of x 
- Compact notation with inner product notation as $\hat{f}(x) = f(z) + \nabla f(z)^T(x-z)$ where $\nabla f(z)$ is the gradient of $f$ as the point z. 
- With this notation we see that the first term is the constant $f(z)$ which is the value of the function when x = z. The second term is the inner product of the gradient of f at z and the deviation or perturbation of x from z (e.g. x - z)

```{r}

# consider f(x) = x1 + exp(x2-x1). Taylor approximation near the point z = 1,2
# Taylor definition f(z) + \nabla f(z)^T(x -z)
taylor <- function(x,y){
    # partials
    p1 <- 1 - exp(y-x)
    p2 <- exp(y-x)
    # Taylor expansion
    fhat <- x + exp(y-x) + t(c(p1, p2))%*%(c(x-1, y-2))
    fhat
}

x <- c(1,0.96, 1.10, 0.85, 1.25)
y <- c(2, 1.98, 2.11, 2.05, 2.41)

for(i in 1:length(x)){
    print(taylor(x[i],y[i]))
}
```

### Regression Model 

The affine function of x $\hat{y} = x^T\beta + v$ where $\beta$ is an n-vector and $v$ is a scalar is called a *regression model* 

- $\beta_i$ is the amount by which $\hat{y}$ changes when feature $i$ increases by 1, holding all other features the same. 
-  $v$ is the value of the $\hat{y}$ when all features have value 0 

We can use vector stacking to put the weights and offset into a single parameter vector. 

$\hat{y} = x^Tb + v = \begin{bmatrix} 1 \\ x  \end{bmatrix}^T \begin{bmatrix} v \\ \beta  \end{bmatrix} = \tilde{x}^T\tilde{\beta}$ 

where the first feature always has the value 1. This constant guarantees an intercept. 

### Exericses 

## Chapter 3: Norm and distance 

**Norm**: The Euclidean norm of an n-vector *x* denoted $||x||$ is the square root of the sum of the squares of its elements. 

$$\begin{aligned}
||x|| = \sqrt{x_1^2 + x_2^2 + ... + x_n^2}
\end{aligned}$$

As an example $||\begin{bmatrix} 2 \\ -1 \\2  \end{bmatrix}|| = \sqrt{9} = 3$

```{r}
norm <- function(vec){
    return(sqrt(sum(vec^2)))
}

norm(c(2,-1,2))

```
##### Properties of norm 

- Nonnegative homogeneity. $||\beta x|| = |\beta|||x||$. Multiplying a vector by a scalar multiplies the norm by the absolute value of the scalar. 
- Triangle inequality. $||x + y || \leq ||x|| + ||y||$. The norm of a sum of two vectors is no more than the sum of the norms. 
    - Also known as subadditivity. 
- Nonnegativity. $||x|| > 0$$
- Definiteness. $||x|| = 0 \iff x = 0$

**Root Mean Square Value** The norm is related the RMSE value of an n-vector x is defined as $\sqrt{\frac{x_1^2 + ... + x_n^2}{n}} = \frac{||x||}{\sqrt{n}}$

**Norm of a sum**. The norm of a sum of two vectors x and y 

$$\begin{aligned}
||x + y ||^2 &= (x+y)^T(x+y) \\
||x + y ||^2 &= x^Tx + x^ty + y^Tx + y^Ty \\
||x + y ||^2 &= ||x||^2 + 2x^Ty + ||y||^2 \\
||x + y || &= \sqrt{||x||^2 + 2x^Ty + ||y||^2}
\end{aligned}$$

**Norm of blocked vectors**. For the block vector d = (a,b,c) where a,b,c are vectors. 

$||(a,b,c)|| = \sqrt{||a||^2 + ||b||^2 + ||c||^2} = (||a||, ||b||, ||c||) = ||d||$

In words, the norm of a stacked vector is the norm of the vector formed by the norms of the subvectors. 

**Chebyshev's Inequality**. Suppose that x is an n-vector and that k of its entries satisfy $|x_i| \geq a, a > 0$. Then k of its entries satisfy $x_i^2 \geq a^2$. 

It follows from this that $||x||^2 \geq ka^2$ since k of the numbers in the sum are at least $a^2$ and the other n-k numbers are nonnegative. 

We conclude from this Chebyshev's Inequality $k \leq \frac{||x||^2}{a^2}$. An alternative way to write this is $\frac{k}{n} \leq (\frac{rms(x)}{a})^2$ where k is the number of entries of x with absolute value at least a. The LHS is the fraction of entries of the vector that are at least a in absolute value. 

### Distance 

**Euclidean Distance**: dist(a,b) = $||a - b||$. 

As an example consider the following. $u = \begin{bmatrix} 1.8 \\ 2.0 \\ -3.7 \\ 4.7 \end{bmatrix}$, $v = \begin{bmatrix} 0.6 \\ 2.1 \\ 1.9 \\ -1.4 \end{bmatrix}$, $w = \begin{bmatrix} 2.0 \\ 1.9 \\ -4.0 \\ 4.6 \end{bmatrix}$. 

Then, the distances between pairs are $||u - v|| = 8.368$, $||u - w|| = 0.387$, and $||v - w|| = 8.533$

### Standard Deviation 

**De-meaned vector**: For any vector x, the vector $\tilde{x} = x - avg(x)\textbf{1}$ is the de-meaned vector. 

**Standard Deviation**: The RMS value of the de-meaned vector. Defined as $\sigma_x = \sqrt{\frac{(x_1 - \bar{x})^2 +...+(x_n - \bar{x})^2}{n}}$. 

We can represent this with inner products as $\sigma_x = \frac{||x - (\textbf{1}^Tx/n)\textbf{1}}{\sqrt{n}}$

```{r}
demean <- function(vec){
    mean_x <- mean(vec)
    return(vec - mean_x)
}

std_dev <- function(vec){
    demeaned <- demean(vec)^2
    return(sqrt(sum(demeaned)/length(demeaned)))
}
std_dev(c(1,-2,3,2))

```
##### Properties of Standard Deviation 

- Adding a constant to every entry of a vector does not change its standard deviation 
- Multiplying a vector by a scalar multiplies the standard deviation by the absolut value of the scalar. 

**Standardization** For any de-meaned vector x, dividing by the standard deviation gives us a standardized version of x. 

$z = \frac{1}{\sigma_x}(x - \mu_x\textbf{1})$

### Angle 

**Cauchy-Schwarz Inequality**: $|a^Tb| \leq ||a|| ||b||$ for any n-vectors a and b. 

**Angle between vectors**: The angle between two nonzero vectors a, b is defined by $\theta = arccos(\frac{a^Tb}{||a||||b||})$ where arccos denotes the inverse cosine normalized to lie in the interval $[0, \pi]$. 

This is equivalent to $\theta$ being the unique number in that interval that satisfies $a^Tb = ||a||||b||cos\theta$ 

```{r}
angle <- function(vec1, vec2){
    # Angle function calculates radians 
    return(acos(innerprod(vec1, vec2)/(norm(vec1)*norm(vec2))))
}

angle(c(1,2,-1), c(2,0,-3))
```
**Norms of sum via angles**: For vectors x and y: 

$||x+y||^2 = ||x||^2 + 2x^Ty + ||y||^2 = ||x||^2 + 2||x||||y||cos\theta + ||y||^2$ 

- If x and y are aligned $\theta = 0$ then the norms add. 
- If x and y are orthogonal, then we have the pythogorean theorem for norms. 

**Correlation Coefficient**: Suppose a and b are vectors and we de-mean them. Assuming the de-meaned vectors are not zero, we define the correlation coefficient as. 

$\rho = \frac{\tilde{a}^T\tilde{b}}{||\tilde{a}||||\tilde{b}||}$ 

```{r}
rho <- function(vec1, vec2){
    vec1_t <- demean(vec1)
    vec2_t <- demean(vec2) 
    
    num <- innerprod(vec1_t, vec2_t)
    den <- norm(vec1_t)*norm(vec2_t)
    return(num/den)
    
}

rho(c(1,2,3,4), c(4,5,6,7))
rho(c(1,2,3,4), c(4,7,2,4))
```

**Standard Deviation of a sum**: $\sigma_{a+b} = \sqrt{\sigma_a^2 + 2\rho\sigma_a\sigma_b + \sigma_b^2}$

Note when a and b are uncorrelated ($\rho = 0$) then this reduces to the sum of the standard deviations. 

## Chapter 4: Clustering 

**Clustering**: Grouping or partitioning vectors into groups with the vectors in each group close to each other in some way. 

## Chapter 5: Linear Independence 

### Linear Dependence and Independence 

**Linear Dependence**: A collection of n-vectors $a_1, a_2,..., a_k$ with $k\geq1$ is linearly dependent if $\beta_1a_1 + ... + \beta_ka_k = 0$ holds from some $\beta_1,..., \beta_k$ that are not all zero. 

- When a collection of vectors are linearly dependent, at least one vector can be expressed as a linear combination of the other vectors. 

**Linear Independence** The above definition only holds for when $\beta_1 = \beta_2 = ... = \beta_k = 0$. E.g. the only linear combination of vectors that equals the zero vector is the linear combination with all coefficients = 0. 

**Linear Combination of Independent Vectors**: Suppose we have a vector *x* that is a linear combination of $a_1,..., a_k$. When the vectors $a_1,...,a_k$ are linearly independent then the coefficients that form x are unique. 

### Basis 

**Independence-dimension inequality**: If n-vectors $a_1,...,a_k$ are linearly independent then $k \leq n$. 

- In words "A linearly independent collection of n-vectors can have at most n elements." 

**Basis**: A collection of linearly independent n-vectors. 

- If the n-vectors $a_1, ..., a_n$ are a basis then any n-vector $b$ can be written as a linear combination of them. 
- Since we know any linear combination of linearly independent vectors can only be expressed in one way we have the implication that *any n-vector $b$ can be written in a unique way as a linear combination of a basis $a_1,...,a_n$* 

## Chapter 6: Matrices 

**Matrix**: a rectangular array of numbers written between brackets. The size of a matrix is the number of columns and the number of rows. For example, a matrix with 3 rows and 4 columns is a 3x4 matrix. In general, a matrix of size m x n is called an m x n matrix. 

```{r}
mat <- matrix(data = c(0,1,2,3,4,5,6,7,8), nrow = 3, ncol=3)
mat
```

**Square Matrix**: a matrix that has an equal number of rows and columns. `mat` above is a square matrix. 

**Zero Matrix**: A matrix with all elements set to zero 
```{r}
zero <- matrix(c(rep(0, 9)), nrow = 3, ncol = 3)
zero
```

**Identity Matrix**: A square matrix with diagonal elements equal to 1 and off diagonal elements equal to 0. 

```{r}
identity <- matrix(c(1,0,0,0,1,0,0,0,1), nrow = 3, ncol = 3)
identity
```

**Diagonal matrix**:  A matrix is diagonal if $A_{ij} = 0, \forall i \neq j$ 

```{r}
diag(identity)
```

**Triangular Matrix**: A square matrix A is *upper triangular* if $A_{ij} = 0, i > j$ and is *lower triangular if $A_{ij} = 0, i < j$. 

- Diagonal matrices are both upper and lower triangular. 

```{r}
upper.tri(identity)
lower.tri(identity)
```

### Transpose, Addition, Norm 

Like vectors, we can take the transpose of a matrix. Transposing twice returns original matrix. 

```{r}
mat_t <- t(mat)
mat_t
t(mat_t)
```

Two matrices of the same size can be added together. Addition is element-wise.  

```{r}
mat + mat
mat - identity
```

**Properties of Matrix addition**

- Commutativity: A + B = B + A 
- Associativity: A + (B + C) = (A + B) + C 
- Addition with Zero matrix: A + 0 = A 
- Transpose of sum: $(A + B)^T = A^T + B^T$

Scalar-matrix multiplication is also elementwise and defined similarly as with vectors 

```{r}
-2 * matrix(c(1,9,6,6,3,0), nrow = 3, ncol = 2)
```

**Matrix Norm**: The norm of a matrix ||A|| is the square root of the sum of the squares of its entries 

```{r}
matrixNorm <- function(mat){
    a <- 0 
    for(i in 1:dim(mat)[1]){
        for(j in 1:dim(mat)[2]){
            a <- a + mat[i,j]^2
        }
    }
    return(sqrt(a))
}
matrixNorm(diag(4))
```

### Matrix-vector Multiplication 

The matrix vector product y = Ax is the m-vector y with elements $\sum_{k=1}^n A_{ik}x_k$ 

```{r}
A <- matrix(c(0,-2, 2,1,-1,1), nrow = 2, ncol =3)
x <- c(2,1,-1)
A%*%x

# Get sum of columns 
t(A)%*%c(1,1)

# Get average of columns 
t(A)%*%c(1/3, 1/3)
```

**Linear Dependence of Columns**: The columns of a matrix are linearly dependent (do not have full rank) if $Ax = 0$ for some $x \neq 0$. The columns of a matrix are linearly independent if $Ax = 0 \implies x = 0$

**Properties of matrix-vector multiplication**: 

- multiplication distributes across the vector argument. $A(u + v) = Au + Av$
- multiplication distributes across the matrix argument $(A + B)u = Au + Bu$

## Chapter 10: Matrix-matrix multiplication 

It is possible to multiply two matrices with compatible dimensions 

```{r}
A <- matrix(c(-1.5, 1, 3, -1, 2, 0), nrow = 2, ncol = 3)
B <- matrix(c(-1, 0, 1, -1, -2, 0), nrow = 3, ncol = 2)

A %*% B
```

Multiplying by the identity matrix will return the original matrix 

```{r}
A %*% diag(3)
```

Note that in general matrix multiplication is not commutative 

```{r}
A <- matrix(c(1,9,6,3),2,2)
B <- matrix(c(0,-1, -1, 2), 2, 2)

A%*%B == B%*%A

```

### Properties of Matrix Multiplication 

- Associativity: (AB)C = A(BC). We can write the product as ABC 

```{r}
C <- matrix(c(2,3,4,5), 2,2)

(A%*%B)%*%C == A%*%(B%*%C)

```

- Associativity with scalar multiplication: $\gamma(AB)=(\gamma A)B$ 

```{r}
gamma <- 5

gamma* (A%*%B) == (gamma*A)%*%B
```

- Distributivity with addition: $A(B+C) == (A+B)C = AC + BC$

```{r}
A%*%(B + C) == A%*%B + A%*%C
(A + B)%*%C == A%*%C + B%*%C    
```

- Transpose of product: The transpose of a product is the product of the transposes in opposite order $(AB)^T = B^T A^T$

```{r}
t((A%*%B)) == t(B)%*%t(A)

```

A fact about inner product and matrix-vector products. If A is m x n, x is an n-vector, and y is an m-vector, then $y^T(Ax) = (y^T A)x = (A^Ty)^Tx$. This means that the inner product of y and Ax is equal to the inner product of x and $A^Ty$

```{r}
x <- c(2,4) 
y <- c(5,6)

t(y)%*%(A%*%x) == t((t(A)%*%y))%*%x
```

**Gram Matrix**: For an m x n matrix A, with columns $a_1,...,a_n$ the matrix product $G= A^TA$. 

- Note, using the transpose of product rule $G^T = (A^TA)^T = (A^T)(A^T) = A^TA = G$

### Matrix Power 

If A is a square matrix then we can note $AA = A^2$ and more generally if k and l are positive integers $A^kA^l = A^{k+l}$

```{r}
A <- matrix(c(0,1,0,1,0, 1, 0,0,0,0,0,1,1,0,0,0,0,1,0,1, 1,0,01,0,0),5,5)
A%*%A
```

### QR Factorization 

With a Gram matrix, we can express the condition that the n-vectors are orthonormal in notation by $A^TA = I$. A square matrix that satisfies this property is called **orthogonal** and its columns are an orthonormal basis. 

PUT IN QR FACTORIZATION R CODE HERE 

```{r}
# QR Factorization 

```

## Chapter 11: Matrix Inverses 

**Left Inverse**: A matrix X that satisfies $XA = I$ is called a left inverse of A. The matrix is said to be *left invertible* if a left inverse exists. 

- If A has a left inverse C then the columns of A are linearly independent. The converse is also true. A matrix has a left inverse iff its columns are linearly independent.

**Right Inverse**: A matrix A that satisfies $AX = I$ is called a right inverse of A. The matrix is *right invertible* if a right inverse exists. 

- A matrix is right invertible iff its rows are linearly independent. 
- Only square or wide matrices can be right invertible 
    - A right inverse can be used to find a solution of a square or underdetermined set of linear equations, for any vector b 
    
### Inverse 

If a matrix is left and right invertible then the left and right inverses are unique and equal. When a matrix has both a left and right inverse then it is *nonsingular* 

- Inverible matrices must be square. A matrix A and its inverse (if it exists) satisfies $AA{^-1} = A^{-1}A = I$
- The square system of linear equations $Ax = b$ with A invertible has the unique solution $x = A^{-1}b$ for any n-vector b. 

For a square matrix the following are equivalent: 

1. A is invertible 
2. The columns of A are linearly independent 
3. The rows of A are linearly independent 
4. A has a left inverse 
5. A has a right inverse 

```{r}
# Example invertible matrix 

A <- matrix(c(1,0,-3, -2, 2, -4, 3,2,-4), nrow = 3, ncol = 3)
round(A%*%solve(A))# floating point prevents this from being exactly zero otherwise 
```

Triangular matrices with nonzero diagonal elements are invertible 

```{r}
solve(diag(3)) # note this also shows identity matrix is invertible

```

### Solving linear equations 

#### Back Substitution 

Given a $n \times n$ upper triangular matrix R with nonzero diagonal entries, and an n-vector $b$. 

For $i = n,...,1$, 
    $x_i = (b_i = R{i, {i+1}}x_{i+1}-...-R_{i,{n}}x_n))/R_{ii}$

For lower triangular matrices with non-zero diagonal elements we can solve using forward substitution, which is the reverse of this process. 

```{r}
r <- rbind(c(1,2,3),
           c(0,1,1),
           c(0,0,2))
y <- backsolve(r, x <- c(8,4,2))
y

```

## Chapter 12: Least Squares 

Suppose that the $m \times n$ matrix A is tall so that system of linear equations $Ax = b$ where $b$ is an m-vector is over-determined. Such an equation only has a solution if $b$ is a linear combination of the columns of A. 

- This rarely happens in practice, so we seek an x for which $r = Ax -b$ is as small as possible. $r = Ax -b$ is referred to as the *residual* 
- The best way to do this is to minimize the norm of the residual $||Ax - b||$ 

Minimizing the norm of a residual and the square of a norm are the same problem, so we minimize $||Ax - b||^2$. 

This approximation is often referred to as *linear least squares regression*

### Solving a Least Squares Problem 

We assume that the columns of A are full rank (e.g. linearly independent)

#### Via Calculus 

Minimizing $||Ax -b ||^2$ must satsify $\frac{\partial f}{\partial x_i} = 0$. 
We can express this as the vector equation $\nabla f(\hat{x}) = 0$ where the LHS is the gradient of $f$ evaluated at $\hat{x}$. In matrix form this is $\nabla f(\hat{x}) = 2A^T(Ax-b)$. 

We can verify this is true by deriving that from scratch. The least squares object as a sum is $f(x) = ||Ax-b||^2 = \sum_{i=1}^m (\sum_{j=1}^n A_{ij}x_j - b_i)^2$ 

Take the partial derivative of $f$ with respect to $x_k$. Differentiating by terms we get. 

$$\begin{aligned}
\nabla f(x)_k &= \frac{\partial f}{\partial x_k}(x) \\
\nabla f(x)_k &= \sum_{i=1}^m 2(\sum_{j=1}^nA_{ij}x_j -b_i)(A_{ik}) \\
\nabla f(x)_k &= \sum_{i=1}^m 2(A^T)_{ki}(Ax-b)_i \\
\nabla f(x)_k &=  (2A^T(Ax-b))_k
\end{aligned}$$

Any minimizer of that must be equal to zero which we can write as $A^TA\hat{x} = A^Tb$. These are the normal equations. Because $A^TA$ is a Gram matrix and we have assumed full rank, it is invertible. This implies. 

$\hat{x} = (A^TA)^{-1}A^Tb$ is the only solution of the normal equations and is therefore unique. 

#### Direct Verification 

We will directly show the proceeding derivation without calculus by showing that for any $x \neq \hat{x}$ we have $||A\hat{x}-b||^2 < ||Ax-b||^2$ and that will establish that $\hat{x}$ is the unique vector that minimizes our problem. 

$$\begin{aligned}
||Ax-b||^2 &= ||(Ax-A\hat{x}) + (A\hat{x} = b)||^2 \\
||Ax-b||^2 &= ||Ax-A\hat{x}||^2 + ||A\hat{x} - b||^2 + 2(Ax - A\hat{x})^T(A\hat{x}-b)\\
||Ax-b||^2 &= ||Ax-A\hat{x}||^2 + ||A\hat{x} - b||^2 + (x-\hat{x})^TA^T(A\hat{x} - b)\\
||Ax-b||^2 &= ||Ax-A\hat{x}||^2 + ||A\hat{x} - b||^2 + (x-\hat{x})^T(A^TA\hat{x}-A^Tb) \\
||Ax-b||^2 &= ||Ax-A\hat{x}||^2 + ||A\hat{x} - b||^2 + (x - \hat{x})^T(A^TA\hat{x}- A^Tb) \\
||Ax-b||^2 &= ||Ax-A\hat{x}||^2 + ||A\hat{x} - b||^2 + (x-\hat{x})0\\
||Ax-b||^2 &= ||Ax-A\hat{x}||^2 + ||A\hat{x} - b||^2
\end{aligned}$$

Note that we made use of an inequality by plugging in the normal equations to make the 3rd term disappear. We are left with an equation that shows that $||A\hat{x}-b||^2 \leq ||Ax-b||^2$. To show unique minimization suppose the equality holds exactly. 

$$\begin{aligned}
||A\hat{x}-b||^2 \leq ||Ax-b||^2 \\
||A(x-\hat{x})||^2 &= 0 \\
A(x-\hat{x}) &= 0 
\end{aligned}$$

Since A has linearly independent columns $x -\hat{x} = 0$. The only $x$ with the equality property is when $x = \hat{x}$ 

**Orthogonality Principle**: The optimal residual is orthogonal to the columns of A and therefore is orthogonal to any linear combination of the columns of A. 

```{r}
n1 <- c(.97, 1.23, .8, 1.29, 1.1, 0.67, 0.87, 1.1, 1.92, 1.29)
n2 <- c(1.86, 2.18, 1.24, .98, 1.23, 0.34, 0.26, 0.16, .22, .12)
n3 <- c(0.41, .53, .62, .51, .69, .54, .62, .48, .71, .62)
y <- as.matrix(c(rep(1000,10)))

# Note this is just doing the least squares solution. We don't have an intercept here. 
R <- matrix(cbind(n1, n2, n3),10,3)
A <-solve(t(R)%*%R)
beta <- round(A%*%t(R)%*%y)
beta
```

## Chapter 13: Least Squares Data Fitting 